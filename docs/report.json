{
    "num_experiments": 26,
    "experiments": [
        {
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "args": {
                "adafactor": true,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "num_train_epochs": 3,
                "per_device_train_batch_size": 384,
                "per_device_eval_batch_size": 384
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 50,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "vinai/bertweet-base. Dataset: full; Epochs: 3; tokenizer_len: 50",
            "results": {
                "accuracy": 0.888752
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/logging/vinai/bertweet-base/20210704-164012/checkpoints/checkpoint-15627",
            "time_stamp": "2021-07-04 21:59:18.156379"
        },
        {
            "model_name_or_path": "siebert/sentiment-roberta-large-english",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "args": {
                "epochs": 5,
                "batch_size": 128,
                "adafactor": true,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": true,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": false
                }
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 50,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "siebert/sentiment-roberta-large-english. Dataset: full; Epochs: 5; tokenizer_len: 50; unfrozen 1 layer; frozen embeddings",
            "results": {
                "accuracy": 0.857706
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/logging/siebert/sentiment-roberta-large-english/20210704-234431/checkpoints/checkpoint-62500",
            "time_stamp": "2021-07-05 06:57:18.845612"
        },
        {
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "args": {
                "adafactor": true,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "num_train_epochs": 3,
                "per_device_train_batch_size": 384,
                "per_device_eval_batch_size": 384,
                "fp16": true
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 50,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "vinai/bertweet-base. Dataset: full; Epochs: 3; tokenizer_len: 50; FP16",
            "results": {
                "accuracy": 0.891444
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/logging/vinai/bertweet-base/20210705-131541/checkpoints/checkpoint-15627",
            "time_stamp": "2021-07-05 15:34:46.276563"
        },
        {
            "model_name_or_path": "distilbert-base-uncased",
            "data_load_ratio": 1.0,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 20,
                "batch_size": 256,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                }
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 50,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "distilbert/base-uncased with hyperopt. Dataset: full; Epochs: 20; tokenizer_len: 50",
            "results": {
                "accuracy": 0.8869597847859751
            },
            "output_dir": "/cluster/home/iathanasi/Computational-Intelligence-Lab/trainings/distilbert-base-uncased/20210708-023219",
            "time_stamp": "2021-07-08 13:09:51.966569"
        },
        {
            "model_name_or_path": "roberta-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 10,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                }
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "roberta-base. Dataset: full; Epochs: 10; tokenizer_len: 64",
            "results": {
                "accuracy": 0.8984093259410769
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/roberta-base/20210709-102233",
            "time_stamp": "2021-07-09 18:22:51.592077"
        },
        {
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 10,
                "batch_size": 256,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": true,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                }
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "vinai/bertweet-base. Dataset: full; Epochs: 10; tokenizer_len: 64; unfrozen 1 layer; unfrozen embeddings",
            "results": {
                "accuracy": 0.8934341752683569
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/vinai/bertweet-base/20210709-101103",
            "time_stamp": "2021-07-09 18:40:35.764386"
        },
        {
            "description": "GPT2. Dataset: full; Epochs: 5; tokenizer_len: 64",
            "model_name_or_path": "gpt2",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "strip",
            "args": {
                "epochs": 5,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "accuracy"
            ],
            "results": {
                "accuracy": 0.8872099517107366
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/gpt2/20210709-212243",
            "time_stamp": "2021-07-10 07:48:12.271636"
        },
        {
            "description": "distilGPT2. Dataset: full; Epochs: 16; tokenizer_len: 64",
            "model_name_or_path": "distilgpt2",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "strip",
            "args": {
                "epochs": 16,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "accuracy"
            ],
            "results": {
                "accuracy": 0.8845232998779115
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/distilgpt2/20210709-211913",
            "time_stamp": "2021-07-10 10:01:31.012450"
        },
        {
            "description": "xlnet-base. Dataset: full; Epochs: 10; tokenizer_len: 64",
            "model_name_or_path": "xlnet-base-cased",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 10,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "results": {
                "accuracy": 0.8947378620593671
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/xlnet-base-cased/20210710-011657",
            "time_stamp": "2021-07-10 13:25:27.705332"
        },
        {
            "description": "GPT2. Dataset: full; Epochs: 10; tokenizer_len: 64",
            "model_name_or_path": "gpt2",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "strip",
            "args": {
                "epochs": 10,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "accuracy"
            ],
            "results": {
                "accuracy": 0.8892817566650987
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/gpt2/20210710-093449",
            "time_stamp": "2021-07-10 22:32:30.012919"
        },
        {
            "description": "GPT2. Dataset: full; Epochs: 10; tokenizer_len: 64; fast_tokenizer",
            "model_name_or_path": "gpt2",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": true,
            "text_pre_cleaning": "strip",
            "args": {
                "epochs": 10,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "accuracy"
            ],
            "results": {
                "accuracy": 0.8895424940233008
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/gpt2/20210710-093448",
            "time_stamp": "2021-07-11 01:11:19.054643"
        },
        {
            "description": "roberta-base. model_length -> 64",
            "model_name_or_path": "roberta-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 10,
                "batch_size": 32,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "results": {
                "accuracy": 0.8971743469674307
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/roberta-base/20210711-135623",
            "time_stamp": "2021-07-12 01:28:00.247066"
        },
        {
            "description": "vinai/bertweet-base. model_length -> 64",
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 10,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "results": {
                "accuracy": 0.9090167558987423
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/vinai/bertweet-base/20210711-131720",
            "time_stamp": "2021-07-12 01:40:17.078373"
        },
        {
            "description": "microsoft_mpnet-base. Dataset: full; Epochs: 10; tokenizer_len: 50; learning_rate: 1e-4, weight_decay: 0.0001",
            "results": {
                "accuracy": 0.8922567346147747
            },
            "output_dir": "/cluster/home/iathanasi/Computational-Intelligence-Lab/trainings/microsoft/mpnet-base/20210713-143207",
            "time_stamp": "2021-07-14 06:16:12.934432",
            "stopped_epoch": 6.0,
            "model_name_or_path": "microsoft/mpnet-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 10,
                "batch_size": 256,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 0.0001,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 50,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "vinai/bertweet-base. FP16, tweet clean (old version)",
            "results": {
                "accuracy": 0.908460332282105
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/vinai/bertweet-base/20210719-100012",
            "time_stamp": "2021-07-19 18:58:18.693769",
            "stopped_epoch": 6.0,
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "tweet",
            "args": {
                "epochs": 8,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 1e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": true,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "microsoft/mpnet-base, FP32, pre-cleaned dataset ",
            "results": {
                "accuracy": 0.8885903557145123
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/microsoft/mpnet-base/20210720-021536",
            "time_stamp": "2021-07-20 13:46:18.547281",
            "stopped_epoch": 5.0,
            "model_name_or_path": "microsoft/mpnet-base",
            "data_load_ratio": "clean",
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "strip",
            "args": {
                "epochs": 10,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 5e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 5e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": false,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "vinai/bertweet-base, FP16, pre-cleaned dataset",
            "results": {
                "accuracy": 0.9014024207614652
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/vinai/bertweet-base/20210720-155034",
            "time_stamp": "2021-07-20 22:53:42.554402",
            "stopped_epoch": 5.0,
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": "clean",
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "strip",
            "args": {
                "epochs": 10,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 5e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": true,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "vinai/bertweet-base, FP32, pre-cleaned dataset",
            "results": {
                "accuracy": 0.9017882626543808
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/vinai/bertweet-base/20210720-155040",
            "time_stamp": "2021-07-21 02:43:13.306270",
            "stopped_epoch": 5.0,
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": "clean",
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "strip",
            "args": {
                "epochs": 7,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 5e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": false,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "vinai/bertweet-base, FP16",
            "results": {
                "accuracy": 0.9086735143325287
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/vinai/bertweet-base/20210721-024602",
            "time_stamp": "2021-07-21 11:00:53.791348",
            "stopped_epoch": 6.0,
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": 1.0,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "tweet",
            "args": {
                "epochs": 10,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 5e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": true,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "roberta-base. FP16",
            "results": {
                "accuracy": 0.8968286967705562
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/roberta-base/20210721-030010",
            "time_stamp": "2021-07-21 13:05:17.036470",
            "stopped_epoch": 8.0,
            "model_name_or_path": "roberta-base",
            "data_load_ratio": 1.0,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "tweet",
            "args": {
                "epochs": 10,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 5e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": true,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "xlnet-base, FP16",
            "results": {
                "accuracy": 0.8908226008210152
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/xlnet-base-cased/20210721-024833",
            "time_stamp": "2021-07-21 15:56:50.955647",
            "stopped_epoch": 7.0,
            "model_name_or_path": "xlnet-base-cased",
            "data_load_ratio": 1.0,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "tweet",
            "args": {
                "epochs": 7,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 5e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 5e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": true,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "vinai/bertweet-base, FP32",
            "results": {
                "accuracy": 0.908264768582956
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/vinai/bertweet-base/20210721-024634",
            "time_stamp": "2021-07-21 16:59:27.977914",
            "stopped_epoch": 6.0,
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": 1.0,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "tweet",
            "args": {
                "epochs": 10,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 5e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": false,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "microsoft/mpnet-base, FP32",
            "results": {
                "accuracy": 0.8947691115065453
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/microsoft/mpnet-base/20210721-024804",
            "time_stamp": "2021-07-21 20:10:42.148159",
            "stopped_epoch": 7.0,
            "model_name_or_path": "microsoft/mpnet-base",
            "data_load_ratio": 1.0,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "tweet",
            "args": {
                "epochs": 10,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 5e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 5e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": false,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "roberta-base. FP16",
            "results": {
                "accuracy": 0.8969855000969009
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/roberta-base/20210721-030005",
            "time_stamp": "2021-07-21 20:57:33.817253",
            "stopped_epoch": 8.0,
            "model_name_or_path": "roberta-base",
            "data_load_ratio": 1.0,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "tweet",
            "args": {
                "epochs": 10,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 5e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": false,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "xlnet-base, FP32",
            "results": {
                "accuracy": 0.8848852164414454
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/xlnet-base-cased/20210721-024833",
            "time_stamp": "2021-07-21 23:43:44.646225",
            "stopped_epoch": 7.0,
            "model_name_or_path": "xlnet-base-cased",
            "data_load_ratio": 1.0,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "tweet",
            "args": {
                "epochs": 7,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 5e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 5e-05,
                "train_val_split_iterator": "train_test_split",
                "fp16": false,
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        },
        {
            "description": "vinai/bertweet-base. dev: data augmentation",
            "results": {
                "accuracy": 0.938028404404899
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/vinai/bertweet-base/20210725-030916",
            "time_stamp": "2021-07-26 00:53:24.531407",
            "stopped_epoch": 10.0,
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": "aug",
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 10,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "max_length": 64,
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ]
        }
    ]
}