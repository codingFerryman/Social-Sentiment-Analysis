{
    "num_experiments": 10,
    "experiments": [
        {
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "args": {
                "adafactor": true,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "num_train_epochs": 3,
                "per_device_train_batch_size": 384,
                "per_device_eval_batch_size": 384
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 50,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "vinai/bertweet-base. Dataset: full; Epochs: 3; tokenizer_len: 50",
            "results": {
                "accuracy": 0.888752
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/logging/vinai/bertweet-base/20210704-164012/checkpoints/checkpoint-15627",
            "time_stamp": "2021-07-04 21:59:18.156379"
        },
        {
            "model_name_or_path": "siebert/sentiment-roberta-large-english",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "args": {
                "epochs": 5,
                "batch_size": 128,
                "adafactor": true,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": true,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": false
                }
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 50,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "siebert/sentiment-roberta-large-english. Dataset: full; Epochs: 5; tokenizer_len: 50; unfrozen 1 layer; frozen embeddings",
            "results": {
                "accuracy": 0.857706
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/logging/siebert/sentiment-roberta-large-english/20210704-234431/checkpoints/checkpoint-62500",
            "time_stamp": "2021-07-05 06:57:18.845612"
        },
        {
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "args": {
                "adafactor": true,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "num_train_epochs": 3,
                "per_device_train_batch_size": 384,
                "per_device_eval_batch_size": 384,
                "fp16": true
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 50,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "vinai/bertweet-base. Dataset: full; Epochs: 3; tokenizer_len: 50; FP16",
            "results": {
                "accuracy": 0.891444
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/logging/vinai/bertweet-base/20210705-131541/checkpoints/checkpoint-15627",
            "time_stamp": "2021-07-05 15:34:46.276563"
        },
        {
            "model_name_or_path": "distilbert-base-uncased",
            "data_load_ratio": 1.0,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 20,
                "batch_size": 256,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                }
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 50,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "distilbert/base-uncased with hyperopt. Dataset: full; Epochs: 20; tokenizer_len: 50",
            "results": {
                "accuracy": 0.8869597847859751
            },
            "output_dir": "/cluster/home/iathanasi/Computational-Intelligence-Lab/trainings/distilbert-base-uncased/20210708-023219",
            "time_stamp": "2021-07-08 13:09:51.966569"
        },
        {
            "model_name_or_path": "roberta-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 10,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                }
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "roberta-base. Dataset: full; Epochs: 10; tokenizer_len: 64",
            "results": {
                "accuracy": 0.8984093259410769
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/roberta-base/20210709-102233",
            "time_stamp": "2021-07-09 18:22:51.592077"
        },
        {
            "model_name_or_path": "vinai/bertweet-base",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 10,
                "batch_size": 256,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "steps",
                "logging_steps": 100,
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": true,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                }
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "description": "vinai/bertweet-base. Dataset: full; Epochs: 10; tokenizer_len: 64; unfrozen 1 layer; unfrozen embeddings",
            "results": {
                "accuracy": 0.8934341752683569
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/vinai/bertweet-base/20210709-101103",
            "time_stamp": "2021-07-09 18:40:35.764386"
        },
        {
            "description": "GPT2. Dataset: full; Epochs: 5; tokenizer_len: 64",
            "model_name_or_path": "gpt2",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "strip",
            "args": {
                "epochs": 5,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "accuracy"
            ],
            "results": {
                "accuracy": 0.8872099517107366
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/gpt2/20210709-212243",
            "time_stamp": "2021-07-10 07:48:12.271636"
        },
        {
            "description": "distilGPT2. Dataset: full; Epochs: 16; tokenizer_len: 64",
            "model_name_or_path": "distilgpt2",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "strip",
            "args": {
                "epochs": 16,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "accuracy"
            ],
            "results": {
                "accuracy": 0.8845232998779115
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/distilgpt2/20210709-211913",
            "time_stamp": "2021-07-10 10:01:31.012450"
        },
        {
            "description": "xlnet-base. Dataset: full; Epochs: 10; tokenizer_len: 64",
            "model_name_or_path": "xlnet-base-cased",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "args": {
                "epochs": 10,
                "batch_size": 128,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "glue",
                "mrpc"
            ],
            "results": {
                "accuracy": 0.8947378620593671
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/xlnet-base-cased/20210710-011657",
            "time_stamp": "2021-07-10 13:25:27.705332"
        },
        {
            "description": "GPT2. Dataset: full; Epochs: 10; tokenizer_len: 64",
            "model_name_or_path": "gpt2",
            "data_load_ratio": 1,
            "model_type": "transformers",
            "tokenizer_type": "transformers",
            "fast_tokenizer": false,
            "text_pre_cleaning": "strip",
            "args": {
                "epochs": 10,
                "batch_size": 64,
                "adafactor": false,
                "warmup_steps": 100,
                "weight_decay": 0.0001,
                "learning_rate": 1e-05,
                "evaluation_strategy": "epoch",
                "logging_strategy": "epoch",
                "overwrite_output_dir": true,
                "load_best_model_at_end": true,
                "metric_for_best_model": "accuracy",
                "early_stopping_patience": 3,
                "early_stopping_threshold": 0.0001,
                "train_val_split_iterator": "train_test_split",
                "fine_tune_layers": {
                    "freeze": false,
                    "num_unfrozen_layers": 1,
                    "unfrozen_embeddings": true
                },
                "report_to": null
            },
            "tokenizer_config": {
                "add_special_tokens": true,
                "max_length": 64,
                "padding": "max_length",
                "truncation": true,
                "return_token_type_ids": true,
                "return_attention_mask": true
            },
            "model_config": {
                "num_labels": 2,
                "problem_type": "single_label_classification",
                "output_attentions": false,
                "output_hidden_states": false,
                "id2label": {
                    "1": 1,
                    "0": -1
                }
            },
            "metric": [
                "accuracy"
            ],
            "results": {
                "accuracy": 0.8892817566650987
            },
            "output_dir": "/cluster/home/heliuhe/cil-project/trainings/gpt2/20210710-093449",
            "time_stamp": "2021-07-10 22:32:30.012919"
        }
    ]
}
