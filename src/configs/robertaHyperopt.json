{
    "model_name_or_path": "roberta-base",
    "data_load_ratio": 0.0001,
    "model_type": "transformers",
    "tokenizer_type": "transformers",
    "args": {
        "epochs": 2,
        "batch_size": {"use_hyperopt": true, "hyperopt_function":"choice", "arguments": {"options": [8,16,32,64]}},
        "warmup_steps": {"use_hyperopt": true, "hyperopt_function":"choice", "arguments": {"options": [20]}},
        "weight_decay": 0.0001,
        "train_val_split_iterator" : "stratifiedKfold",
        "evaluation_strategy": "epoch",
        "logging_strategy": "steps",
        "logging_steps": 100,
        "overwrite_output_dir": true,
        "load_best_model_at_end": true,
        "metric_for_best_model": "accuracy",
        "fine_tune_layers": {
            "freeze": true,
            "num_unfrozen_layers": 0,
            "unfrozen_embeddings": false
        },
        "adafactor": true,
        "learning_rate": 1e-5
    },
    "tokenizer_config": {
        "add_special_tokens": true,
        "max_length": 32,
        "padding": "max_length",
        "truncation": true,
        "return_token_type_ids": true,
        "return_attention_mask": true
    },
    "model_config": {
        "num_labels": 2,
        "problem_type": "single_label_classification",
        "output_attentions": false,
        "output_hidden_states": false
    },
    "metric": [
        "glue",
        "mrpc"
    ],
    "metrics": [],
    "use_hyperopt": true,
    "hyperopt_max_evals": 2
}